diff --git a/AGENTS.md b/AGENTS.md
new file mode 100644
index 0000000000000000000000000000000000000000..8446dc0ac1126f75b98788e30b5ab7bd40ec9910
--- /dev/null
+++ b/AGENTS.md
@@ -0,0 +1,66 @@
+# AGENT Notes
+
+This repository implements the **AI Scientist v2** project. It contains tools for automated scientific discovery via agentic tree search and includes a minimal `paper_improver` subpackage for iteratively refining LaTeX papers.
+
+Below is an overview of the repository structure with short descriptions of the most relevant modules and scripts. This file is intended for Codex agents working on the repository.
+
+## Top-Level Layout
+
+```
+AI-Scientist-v2/
+‚îú‚îÄ‚îÄ ai_scientist/           # Main Python package
+‚îú‚îÄ‚îÄ docs/                   # Documentation assets
+‚îú‚îÄ‚îÄ examples/               # Example data for quick runs
+‚îú‚îÄ‚îÄ scripts/                # Command line entry points
+‚îú‚îÄ‚îÄ tests/                  # Pytest suite
+‚îú‚îÄ‚îÄ launch_scientist_bfts.py# Full experiment launcher
+‚îú‚îÄ‚îÄ bfts_config.yaml        # Default config for tree search
+‚îî‚îÄ‚îÄ README.md               # User-facing instructions
+```
+
+### Root Scripts
+- **`launch_scientist_bfts.py`** ‚Äì The original entry point for tree-search experiments. It orchestrates ideation, experiment execution, plotting, write-up generation, and peer reviews. It relies heavily on modules under `ai_scientist/treesearch`.
+- **`scripts/launch_paper_improver.py`** ‚Äì Simplified CLI for the `paper_improver` package. It accepts paths to a LaTeX project, seed ideas, optional human reviews, and various model names/keys.
+
+## `ai_scientist/` Package
+This package houses all functionality. Notable submodules include:
+
+- **`llm.py`** ‚Äì Helpers to create clients for OpenAI, Gemini, or Claude models and wrappers for chat completions.
+- **`vlm.py`** ‚Äì Utilities for vision-language models and image handling.
+- **`perform_ideation_temp_free.py`** ‚Äì Generates research ideas from a Markdown prompt using LLMs and the Semantic Scholar tool.
+- **`perform_icbinb_writeup.py`** and **`perform_writeup.py`** ‚Äì LLM-driven LaTeX generation utilities. They also contain `compile_latex` for rendering PDFs.
+- **`perform_llm_review.py`** and **`perform_vlm_review.py`** ‚Äì Modules to review papers or code with LLM/VLM models.
+- **`tools/`** ‚Äì Contains utilities such as `semantic_scholar.py` for literature search.
+- **`treesearch/`** ‚Äì The main agentic tree-search implementation. It defines the `Journal`, `Node`, backends for LLM calls, and utilities for exploring code versions. The `perform_experiments_bfts_with_agentmanager.py` module launches the full search process.
+- **`paper_improver/`** ‚Äì The new package providing a minimal pipeline for improving LaTeX papers without running experiments (described in detail below).
+
+## `paper_improver` Subpackage
+This folder contains a simplified pipeline that reuses LLM/VLM review utilities and the tree-search style search loop. Key modules:
+
+- **`__init__.py`** ‚Äì Re-exports `improve_paper` for convenience.
+- **`latex_editor.py`** ‚Äì Uses an LLM to propose and apply edits to a LaTeX file, extracting updated code from a fenced ` ```latex` block.
+- **`llm_review.py`** / **`vlm_review.py`** ‚Äì Thin wrappers around `perform_llm_review.perform_review` and `perform_vlm_review.perform_imgs_cap_ref_review`.
+- **`meta_review.py`** ‚Äì Aggregates multiple review JSON objects into a single numerical score.
+- **`search.py`** ‚Äì Implements two search strategies:
+  - `breadth_first_improve` ‚Äì Basic breadth-first search over paper versions.
+  - `tree_search_improve` ‚Äì Priority-based search mirroring the main repository‚Äôs tree search but omitting experiment execution.
+  - Defines `PaperNode`, `Journal`, and `ORCHESTRATOR_MODEL` for orchestrated selection of the best node.
+- **`pipeline.py`** ‚Äì High-level function `improve_paper` that chooses the search strategy and passes model names along.
+- **`utils.py`** ‚Äì Helper functions like `unique_subdir` for creating non-colliding directories.
+
+The `examples/paper_improver_minimal/` directory includes a sample LaTeX project, seed ideas JSON, and human reviews to demonstrate the pipeline.
+
+## Testing
+The `tests/` directory provides a small pytest suite. `conftest.py` stubs heavy dependencies (LLM clients, token tracker, tree-search backend) so that tests run quickly offline. Tests cover `meta_review`, the pipeline‚Äôs strategy selection, and core search logic by patching out network calls.
+
+To run all tests:
+```bash
+pytest -q
+```
+
+## Development Tips
+- Most modules rely on environment variables for API keys (`OPENAI_API_KEY`, `GEMINI_API_KEY`, etc.). The CLI scripts also allow passing keys as arguments.
+- Tree search results (when using the full `launch_scientist_bfts.py` pipeline) are stored under `experiments/` in timestamped folders with logs and HTML visualizations.
+- The repository‚Äôs Python code is formatted with `black` and tests assume Python 3.11.
+
+This overview should help orient new Codex agents when making modifications or adding features.
diff --git a/README.md b/README.md
index 525e8f51b791647c36d44f430246131fcdd6057c..5acba81d2bd3164478c6acf897be5c88759ad128 100644
--- a/README.md
+++ b/README.md
@@ -5,58 +5,59 @@
   <h1>
     <b>The AI Scientist-v2: Workshop-Level Automated</b><br>
     <b>Scientific Discovery via Agentic Tree Search</b>
   </h1>
 </div>
 
 <p align="center">
   üìö <a href="https://pub.sakana.ai/ai-scientist-v2/paper">[Paper]</a> |
   üìù <a href="https://sakana.ai/ai-scientist-first-publication/"> [Blog Post]</a> |
   üìÇ <a href="https://github.com/SakanaAI/AI-Scientist-ICLR2025-Workshop-Experiment"> [ICLR2025 Workshop Experiment]</a>
 </p>
 
 Fully autonomous scientific research systems are becoming increasingly capable, with AI playing a pivotal role in transforming how scientific discoveries are made.
 We are excited to introduce The AI Scientist-v2, a generalized end-to-end agentic system that has generated the first workshop paper written entirely by AI and accepted through peer review.
 
 This system autonomously generates hypotheses, runs experiments, analyzes data, and writes scientific manuscripts. Unlike [its predecessor (AI Scientist-v1)](https://github.com/SakanaAI/AI-Scientist), the AI Scientist-v2 removes reliance on human-authored templates, generalizes across Machine Learning (ML) domains, and employs a progressive agentic tree search, guided by an experiment manager agent.
 
 > **Note:**
 > The AI Scientist-v2 doesn‚Äôt necessarily produce better papers than v1, especially when a strong starting template is available. v1 follows well-defined templates, leading to high success rates, while v2 takes a broader, more exploratory approach with lower success rates. v1 works best for tasks with clear objectives and a solid foundation, whereas v2 is designed for open-ended scientific exploration.
 
 > **Caution!**
 > This codebase will execute Large Language Model (LLM)-written code. There are various risks and challenges associated with this autonomy, including the potential use of dangerous packages, uncontrolled web access, and the possibility of spawning unintended processes. Ensure that you run this within a controlled sandbox environment (e.g., a Docker container). Use at your own discretion.
 
 ## Table of Contents
 
-1.  [Requirements](#requirements)
-    *   [Installation](#installation)
-    *   [Supported Models and API Keys](#supported-models-and-api-keys)
-2.  [Generate Research Ideas](#generate-research-ideas)
-3.  [Run AI Scientist-v2 Paper Generation Experiments](#run-ai-scientist-v2-paper-generation-experiments)
-4.  [Citing The AI Scientist-v2](#citing-the-ai-scientist-v2)
-5.  [Frequently Asked Questions](#frequently-asked-questions)
-6.  [Acknowledgement](#acknowledgement)
+- [Requirements](#requirements)
+  - [Installation](#installation)
+  - [Supported Models and API Keys](#supported-models-and-api-keys)
+- [Generate Research Ideas](#generate-research-ideas)
+- [Run AI Scientist-v2 Paper Generation Experiments](#run-ai-scientist-v2-paper-generation-experiments)
+- [Minimal Paper Improver Example](#minimal-paper-improver-example)
+- [Citing The AI Scientist-v2](#citing-the-ai-scientist-v2)
+- [Frequently Asked Questions](#frequently-asked-questions)
+- [Acknowledgement](#acknowledgement)
 
 ## Requirements
 
 This code is designed to run on Linux with NVIDIA GPUs using CUDA and PyTorch.
 
 ### Installation
 
 ```bash
 # Create a new conda environment
 conda create -n ai_scientist python=3.11
 conda activate ai_scientist
 
 # Install PyTorch with CUDA support (adjust pytorch-cuda version for your setup)
 conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia
 
 # Install PDF and LaTeX tools
 conda install anaconda::poppler
 conda install conda-forge::chktex
 
 # Install Python package requirements
 pip install -r requirements.txt
 ```
 
 ### Supported Models and API Keys
 
@@ -130,50 +131,74 @@ Key tree search configuration parameters in `bfts_config.yaml`:
 -   `agent` config:
     -   Set `num_workers` (number of parallel exploration paths) and `steps` (maximum number of nodes to explore). For example, if `num_workers=3` and `steps=21`, the tree search will explore up to 21 nodes, expanding 3 nodes concurrently at each step.
     -   `num_seeds`: Should generally be the same as `num_workers` if `num_workers` is less than 3. Otherwise, set `num_seeds` to 3.
     -   Note: Other agent parameters like `k_fold_validation`, `expose_prediction`, and `data_preview` are not used in the current version.
 -   `search` config:
     -   `max_debug_depth`: The maximum number of times the agent will attempt to debug a failing node before abandoning that search path.
     -   `debug_prob`: The probability of attempting to debug a failing node.
     -   `num_drafts`: The number of initial root nodes (i.e., the number of independent trees to grow) during Stage 1.
 
 Example command to run AI-Scientist-v2 using a generated idea file (e.g., `my_research_topic.json`). Please review `bfts_config.yaml` for detailed tree search parameters (the default config includes `claude-3-5-sonnet` for experiments). Do not set `load_code` if you do not want to initialize experimentation with a code snippet.
 
 ```bash
 python launch_scientist_bfts.py \
  --load_ideas "ai_scientist/ideas/my_research_topic.json" \
  --load_code \
  --add_dataset_ref \
  --model_writeup o1-preview-2024-09-12 \
  --model_citation gpt-4o-2024-11-20 \
  --model_review gpt-4o-2024-11-20 \
  --model_agg_plots o3-mini-2025-01-31 \
  --num_cite_rounds 20
 ```
 
 Once the initial experimental stage is complete, you will find a timestamped log folder inside the `experiments/` directory. Navigate to `experiments/"timestamp_ideaname"/logs/0-run/` within that folder to find the tree visualization file `unified_tree_viz.html`.
 
+## Minimal Paper Improver Example
+
+For a quick demonstration of the `paper_improver` sub-pipeline, use the sample
+files located in `examples/paper_improver_minimal/`:
+
+```bash
+python scripts/launch_paper_improver.py examples/paper_improver_minimal \
+    examples/paper_improver_minimal/seed_ideas.json \
+    --human-reviews examples/paper_improver_minimal/human_reviews.txt \
+    --max-depth 1 --beam-size 1 \
+    --num-drafts 1 --debug-prob 0.5 --max-debug-depth 3 \
+    --model-editor o1-preview-2024-09-12 \
+    --model-review gpt-4o-2024-11-20 \
+    --model-vlm gpt-4o-2024-11-20 \
+    --model-orchestrator gpt-4o-2024-11-20 \
+    --model-citation gpt-4o-2024-11-20 \
+    --num-cite-rounds 20
+```
+
+You can override API keys at runtime via `--openai-api-key` or `--gemini-api-key`.
+
+This runs a single-depth search over the included LaTeX project and produces an
+improved version inside the same directory.
+
 ## Citing The AI Scientist-v2
 
 If you use **The AI Scientist-v2** in your research, please cite our work as follows:
 
 ```bibtex
 @article{aiscientist_v2,
   title={The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search},
   author={Yamada, Yutaro and Lange, Robert Tjarko and Lu, Cong and Hu, Shengran and Lu, Chris and Foerster, Jakob and Clune, Jeff and Ha, David},
   journal={arXiv preprint arXiv:2504.08066},
   year={2025}
 }
 ```
 
 ## Frequently Asked Questions
 
 **Why wasn't a PDF or a review generated for my experiment?**
 
 The AI Scientist-v2 completes experiments with a success rate that depends on the chosen foundation model, and the complexity of the idea. Higher success rates are generally observed when using powerful models like Claude 3.5 Sonnet for the experimentation phase.
 
 **What is the estimated cost per experiment?**
 
 The ideation step cost depends on the LLM used and the number of generations/reflections, but is generally low (a few dollars). For the main experiment pipeline, using Claude 3.5 Sonnet for the experimentation phase typically costs around $15‚Äì$20 per run. The subsequent writing phase adds approximately $5 when using the default models specified in the example command. Using GPT-4o for `model_citation` is recommended as it can help reduce writing costs.
 
 **How do I run The AI Scientist-v2 for different subject fields?**
 
diff --git a/ai_scientist/paper_improver/__init__.py b/ai_scientist/paper_improver/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..557272064b4c18ba6d154e5472fd4165a9b199f1
--- /dev/null
+++ b/ai_scientist/paper_improver/__init__.py
@@ -0,0 +1,16 @@
+"""Top-level helpers for the :mod:`paper_improver` package.
+
+This module exposes a small public surface consisting of the
+``improve_paper`` pipeline function and ``reflect_paper`` utility.  The
+actual implementation lives in the submodules under ``paper_improver``.
+
+The rest of the package mirrors the larger AI-Scientist tree search but is
+specialised for iteratively polishing LaTeX papers.  ``improve_paper``
+handles high level orchestration while ``reflect_paper`` runs a final
+polishing loop using ``chktex`` and vision-language reviews.
+"""
+
+from .pipeline import improve_paper
+from .reflection import reflect_paper
+
+__all__ = ["improve_paper", "reflect_paper"]
diff --git a/ai_scientist/paper_improver/latex_editor.py b/ai_scientist/paper_improver/latex_editor.py
new file mode 100644
index 0000000000000000000000000000000000000000..8c2f9a7450ab86112e0781a90398cf78498d91fc
--- /dev/null
+++ b/ai_scientist/paper_improver/latex_editor.py
@@ -0,0 +1,70 @@
+"""Utilities for editing LaTeX using a language model.
+
+The ``propose_edit`` function is a very small wrapper around the standard
+``create_client`` helper.  It sends the current LaTeX source, optional
+human reviews and a list of seed ideas to an LLM, then parses the model's
+response for a fenced `````latex`` block.  The extracted code is returned
+without writing it to disk.  Callers are responsible for updating the
+``template.tex`` file themselves.
+"""
+from pathlib import Path
+from ai_scientist.llm import create_client
+from ai_scientist.utils.token_tracker import track_token_usage
+
+EDITOR_MODEL = "o1-preview-2024-09-12"
+
+
+@track_token_usage
+def propose_edit(
+    latex_path: Path,
+    seed_ideas: str,
+    human_reviews: str | None = None,
+    model: str = EDITOR_MODEL,
+) -> str:
+    """Return *new* LaTeX code after applying improvements suggested by the model.
+
+    Parameters
+    ----------
+    latex_path
+        Path to the LaTeX source to edit in place.
+    seed_ideas
+        High-level suggestions guiding the improvement.
+    human_reviews
+        Optional human reviewer comments to address.
+    model
+        Model used to propose edits.
+    """
+    # Build a single prompt containing the source, reviews and improvement ideas
+    # so the model can propose an updated version of the document.
+    prompt = f"""You are an expert academic writing assistant.  Below is the current LaTeX paper, a set of human reviews, and high-level improvement ideas.
+Improve the document **in place** focusing on clarity, scientific rigour, and addressing reviewers‚Äô concerns.  Output *only* the updated LaTeX in a fenced ```latex block.
+
+############  CURRENT LaTeX ############
+{latex_path.read_text()}
+########################################
+
+############ HUMAN REVIEWS ############
+{human_reviews or 'N/A'}
+########################################
+
+############  SEED IDEAS   ############
+{seed_ideas}
+########################################
+"""
+    # Send the prompt to the selected model and obtain a completion.
+    client, m = create_client(model)
+    resp = client.chat.completions.create(
+        model=m,
+        messages=[{"role": "user", "content": prompt}],
+        temperature=0.4,
+    )
+    # Extract LaTeX from fenced block ‚Äì simple heuristic
+    import re, textwrap
+    # The model is instructed to place the revised LaTeX in a fenced block.
+    # A simple non-greedy regex is used to capture that code snippet.
+    code = re.search(r"```latex\s*(.*?)```", resp.choices[0].message.content, re.DOTALL)
+    if not code:
+        raise ValueError("No LaTeX block returned by editor model")
+    # Normalise indentation just in case the model added leading spaces.
+    new_source = textwrap.dedent(code.group(1)).strip()
+    return new_source
diff --git a/ai_scientist/paper_improver/llm_review.py b/ai_scientist/paper_improver/llm_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..0b684515afc8fc5659eaa1929976dfa9e4b43793
--- /dev/null
+++ b/ai_scientist/paper_improver/llm_review.py
@@ -0,0 +1,35 @@
+"""Wrapper around :mod:`perform_llm_review` for paper improvement.
+
+The real heavy lifting happens in :func:`perform_llm_review.perform_review`.
+This module simply exposes a thin convenience function ``llm_review`` that
+forwards the desired model name and any keyword arguments.  ``create_client``
+is used so the same code works for OpenAI, Gemini or Anthropic models.
+"""
+from typing import Any
+from ai_scientist.perform_llm_review import perform_review  # existing util
+from ai_scientist.llm import create_client
+
+DEFAULT_MODEL = "gpt-4o-2024-11-20"
+
+
+def llm_review(
+    tex_or_pdf_path: str,
+    *,
+    model: str = DEFAULT_MODEL,
+    **kwargs,
+) -> dict[str, Any]:
+    """Run the standard LLM review and return the parsed JSON.
+
+    Parameters
+    ----------
+    tex_or_pdf_path
+        Path to the LaTeX source (compiled) *or* an already-built PDF.
+    model
+        LLM to use for the review phase.
+    """
+    client, m = create_client(model)
+    # Delegate the actual reviewing logic to ``perform_review``.  We simply pass
+    # through the client and model name so downstream code remains decoupled
+    # from any particular LLM provider.
+    review_json = perform_review(tex_or_pdf_path, m, client, **kwargs)
+    return review_json
diff --git a/ai_scientist/paper_improver/meta_review.py b/ai_scientist/paper_improver/meta_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..389b43cc5eb4c4c454adabbf067caf97a906f56d
--- /dev/null
+++ b/ai_scientist/paper_improver/meta_review.py
@@ -0,0 +1,42 @@
+"""Utility functions for aggregating review scores.
+
+``score_single`` computes a weighted average over the numeric fields present
+in a single review JSON object.  ``meta_score`` then averages those scores
+across multiple reviewers.  These helpers are intentionally simple; they only
+look at a few high-level categories and normalise the "Overall" field onto the
+same 1‚Äì4 scale as the others.  The resulting float is used by the search code
+to rank paper versions.
+"""
+from __future__ import annotations
+from statistics import mean
+from typing import Sequence, Dict, Any
+
+DEFAULT_WEIGHTS = {
+    "Originality": 3,
+    "Quality": 3,
+    "Clarity": 2,
+    "Significance": 4,
+    "Overall": 5,  # emphasise overall judgement
+}
+
+
+def score_single(review_json: Dict[str, Any], weights: Dict[str, int] | None = None) -> float:
+    """Weighted average of key fields (1-10 scaled to 1-4 where needed)."""
+    if weights is None:
+        weights = DEFAULT_WEIGHTS
+    total, denom = 0.0, 0.0  # running numerator/denominator for the average
+    for k, w in weights.items():
+        val = review_json.get(k)
+        if val is None:
+            continue
+        # normalise different scales
+        if k == "Overall":
+            val = val / 2.5  # map 1-10 ‚Üí ~0-4
+        total += w * float(val)
+        denom += w
+    return total / denom if denom else 0.0
+
+
+def meta_score(reviews: Sequence[Dict[str, Any]]) -> float:
+    """Average ``score_single`` over a set of reviews."""
+    return mean(score_single(r) for r in reviews) if reviews else 0.0
diff --git a/ai_scientist/paper_improver/pipeline.py b/ai_scientist/paper_improver/pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..4a465fef198a41033bba91f17983e3ec2823f6cf
--- /dev/null
+++ b/ai_scientist/paper_improver/pipeline.py
@@ -0,0 +1,120 @@
+"""High level orchestration for the paper improver.
+
+``improve_paper`` is the entry point used by the CLI and tests.  It builds a
+``SearchParams`` instance from the supplied hyper-parameters, chooses the search
+strategy (breadth first or priority tree search) and then performs optional
+citation gathering and reflection on the best node.
+"""
+
+from pathlib import Path
+import logging
+from .search import (
+    breadth_first_improve,
+    tree_search_improve,
+    ORCHESTRATOR_MODEL,
+    SearchParams,
+)
+from .latex_editor import EDITOR_MODEL
+from .llm_review import DEFAULT_MODEL
+from .vlm_review import VLM_MODEL
+from .reflection import (
+    reflect_paper,
+    REFLECTION_MODEL,
+    DEFAULT_ROUNDS as DEFAULT_REFLECTION_ROUNDS,
+    DEFAULT_PAGE_LIMIT,
+)
+from ai_scientist.perform_icbinb_writeup import gather_citations
+
+CITATION_MODEL = "gpt-4o-2024-11-20"
+DEFAULT_CITE_ROUNDS = 20
+REFLECTION_MODEL_DEFAULT = REFLECTION_MODEL
+DEFAULT_REFLECTION_ROUNDS = DEFAULT_REFLECTION_ROUNDS
+DEFAULT_PAGE_LIMIT_VALUE = DEFAULT_PAGE_LIMIT
+
+logger = logging.getLogger(__name__)
+
+
+def improve_paper(
+    latex_project_dir: str | Path,
+    seed_ideas: str,
+    human_reviews: str | None = None,
+    strategy: str = "bfs",
+    model_editor: str = EDITOR_MODEL,
+    model_review: str = DEFAULT_MODEL,
+    model_vlm: str = VLM_MODEL,
+    orchestrator_model: str = ORCHESTRATOR_MODEL,
+    model_citation: str = CITATION_MODEL,
+    num_cite_rounds: int = DEFAULT_CITE_ROUNDS,
+    model_reflection: str = REFLECTION_MODEL_DEFAULT,
+    num_reflections: int = DEFAULT_REFLECTION_ROUNDS,
+    page_limit: int = DEFAULT_PAGE_LIMIT_VALUE,
+    max_depth: int = 3,
+    beam_size: int = 4,
+    num_drafts: int = 3,
+    debug_prob: float = 0.5,
+    max_debug_depth: int = 3,
+    **kwargs,
+):
+    # Resolve paths early to avoid confusion when the working directory changes
+    root = Path(latex_project_dir).resolve()
+
+    # Bundle all search-related hyper parameters into a dataclass instance
+    params = SearchParams(
+        max_depth=max_depth,
+        beam_size=beam_size,
+        num_drafts=num_drafts,
+        debug_prob=debug_prob,
+        max_debug_depth=max_debug_depth,
+    )
+    if strategy == "tree":
+        # Priority-based tree search closely mirrors the main AI Scientist
+        # implementation but skips experiment execution.
+        best_state, _journal = tree_search_improve(
+            root,
+            seed_ideas,
+            human_reviews,
+            params=params,
+            model_editor=model_editor,
+            model_review=model_review,
+            model_vlm=model_vlm,
+            orchestrator_model=orchestrator_model,
+            **kwargs,
+        )
+    else:
+        # The default strategy explores states in a breadth-first manner.
+        best_state, _journal = breadth_first_improve(
+            root,
+            seed_ideas,
+            human_reviews,
+            params=params,
+            model_editor=model_editor,
+            model_review=model_review,
+            model_vlm=model_vlm,
+            orchestrator_model=orchestrator_model,
+            **kwargs,
+        )
+    # Optionally refine citations for the best candidate
+    if num_cite_rounds > 0:
+        logger.info(
+            "Gathering citations with %s for %d rounds", model_citation, num_cite_rounds
+        )
+        gather_citations(
+            best_state.latex_dir,
+            num_cite_rounds=num_cite_rounds,
+            small_model=model_citation,
+        )
+    # Final reflection loop to polish LaTeX and check page limits
+    if num_reflections > 0:
+        logger.info(
+            "Running %d reflection rounds with %s", num_reflections, model_reflection
+        )
+        reflect_paper(
+            best_state.latex_dir,
+            model=model_reflection,
+            vlm_model=model_vlm,
+            num_rounds=num_reflections,
+            page_limit=page_limit,
+        )
+    # Return the path to the best version for further inspection
+    logger.info("Best improved paper saved at %s", best_state.latex_dir)
+    return best_state
diff --git a/ai_scientist/paper_improver/reflection.py b/ai_scientist/paper_improver/reflection.py
new file mode 100644
index 0000000000000000000000000000000000000000..5654e1eee6bc15f8a4a0ddc3d37cceceb5cc1ba0
--- /dev/null
+++ b/ai_scientist/paper_improver/reflection.py
@@ -0,0 +1,113 @@
+"""Utility for post-processing the best paper version.
+
+The ``reflect_paper`` function repeatedly compiles the LaTeX source and feeds
+the resulting PDF through ``chktex`` and a VLM based figure review.  The output
+of these tools is provided to an editing model which returns an updated LaTeX
+snippet.  The loop terminates early if no changes are made or if the page limit
+is exceeded.
+"""
+from __future__ import annotations
+
+import logging
+import os
+import re
+import subprocess
+import textwrap
+from pathlib import Path
+
+from ai_scientist.llm import create_client
+from ai_scientist.perform_icbinb_writeup import (
+    compile_latex,
+    get_reflection_page_info,
+)
+from ai_scientist.perform_vlm_review import (
+    perform_imgs_cap_ref_review,
+    detect_duplicate_figures,
+)
+from ai_scientist.vlm import create_client as create_vlm_client
+
+logger = logging.getLogger(__name__)
+
+REFLECTION_MODEL = "o1-preview-2024-09-12"
+DEFAULT_ROUNDS = 3
+DEFAULT_PAGE_LIMIT = 4
+
+
+def reflect_paper(
+    latex_dir: Path,
+    *,
+    model: str = REFLECTION_MODEL,
+    vlm_model: str = "gpt-4o-2024-11-20",
+    num_rounds: int = DEFAULT_ROUNDS,
+    page_limit: int = DEFAULT_PAGE_LIMIT,
+) -> None:
+    """Run a reflection loop over the LaTeX source to polish the paper."""
+
+    tex_path = latex_dir / "template.tex"  # file that will be repeatedly updated
+    # Create clients for the LLM and VLM models used in this step
+    client, m = create_client(model)
+    vlm_client, vm = create_vlm_client(vlm_model)
+    msg_history: list[dict] = []
+
+    for i in range(num_rounds):
+        # Compile the current LaTeX source to PDF for analysis
+        pdf_path = latex_dir / f"reflection_{i+1}.pdf"
+        compile_latex(str(latex_dir), str(pdf_path))
+
+        try:
+            # ``chktex`` catches common LaTeX style issues. We ignore a few
+            # checks that produce noisy suggestions.
+            res = subprocess.run(
+                ["chktex", str(tex_path), "-q", "-n2", "-n24", "-n13", "-n1"],
+                capture_output=True,
+                text=True,
+            )
+            chk_output = res.stdout
+        except Exception as exc:
+            logger.error("chktex failed: %s", exc)
+            chk_output = ""
+
+        # Vision-language model checks ensure figures and captions are coherent
+        vlm_rev = perform_imgs_cap_ref_review(vlm_client, vm, str(pdf_path))
+        dup_figs = detect_duplicate_figures(vlm_client, vm, str(pdf_path))
+        page_info = get_reflection_page_info(str(pdf_path), page_limit)
+
+        prompt = textwrap.dedent(
+            f"""
+            Now let's reflect on the paper and identify any issues.
+            {page_info}
+            chktex results:
+            ```
+            {chk_output}
+            ```
+            VLM review:
+            ```
+            {vlm_rev}
+            ```
+            Duplicate figures:
+            ```
+            {dup_figs}
+            ```
+            Provide the revised LaTeX in a fenced ```latex block.
+            """
+        )
+
+        # Ask the LLM to produce an improved LaTeX snippet given the diagnostics
+        resp = client.chat.completions.create(
+            model=m,
+            messages=[{"role": "user", "content": prompt}],
+            temperature=0.3,
+        )
+        code = re.search(r"```latex\s*(.*?)```", resp.choices[0].message.content, re.DOTALL)
+        if not code:
+            break
+        new_src = textwrap.dedent(code.group(1)).strip()
+        current = tex_path.read_text()
+        if new_src == current:
+            # Stop if the model produced identical LaTeX; nothing left to refine
+            break
+        tex_path.write_text(new_src)
+
+    # Finally compile once more so the directory contains the latest PDF
+    compile_latex(str(latex_dir), str(latex_dir / "template.pdf"))
+
diff --git a/ai_scientist/paper_improver/search.py b/ai_scientist/paper_improver/search.py
new file mode 100644
index 0000000000000000000000000000000000000000..039d995570c9587dfe69266b67f2555e50ad7124
--- /dev/null
+++ b/ai_scientist/paper_improver/search.py
@@ -0,0 +1,381 @@
+"""Search strategies for iteratively improving a LaTeX paper.
+
+This module implements two algorithms:
+
+``breadth_first_improve`` ‚Äì a simple breadth-first traversal that expands all
+nodes level by level.
+
+``tree_search_improve`` ‚Äì a priority based search that more closely resembles
+the tree search used in the main AI-Scientist pipeline.
+
+Both operate on ``PaperNode`` objects which hold paths to on-disk LaTeX
+projects.  Each node is scored via LLM and VLM reviews and the resulting number
+is used to rank candidates.  The ``Journal`` class records every explored node
+and can optionally defer the final selection to an orchestration model.
+"""
+
+from __future__ import annotations
+
+from collections import deque
+import heapq
+import uuid
+import random
+from dataclasses import dataclass
+from pathlib import Path
+import shutil
+import json
+import logging
+from ai_scientist.treesearch.backend import query, FunctionSpec
+from .latex_editor import propose_edit, EDITOR_MODEL
+from .llm_review import llm_review, DEFAULT_MODEL
+from .vlm_review import vlm_review, VLM_MODEL
+from .meta_review import meta_score
+from .utils import unique_subdir
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class SearchParams:
+    """Hyper-parameters controlling tree search behavior.
+
+    ``max_depth``
+        Maximum depth to explore in the search tree.
+    ``beam_size``
+        Number of children to generate from each node.
+    ``num_drafts``
+        How many initial drafts are spawned before the main loop.
+    ``debug_prob`` and ``max_debug_depth``
+        Control the probabilistic retry mechanism when a node fails to evaluate
+        (e.g. LaTeX compilation errors).
+    """
+
+    max_depth: int = 3
+    beam_size: int = 4
+    num_drafts: int = 3
+    debug_prob: float = 0.5
+    max_debug_depth: int = 3
+
+
+def safe_evaluate(node: "PaperNode") -> float | None:
+    """Evaluate a node and mark it buggy on failure."""
+    try:
+        return node.evaluate()
+    except Exception as exc:
+        # Any exception during ``evaluate`` marks the node as buggy so the
+        # search algorithms can attempt a debug retry.
+        logger.error("Evaluation failed for %s: %s", node.latex_dir, exc)
+        node.is_buggy = True
+        return None
+
+
+# Default model used to pick the best node once search has finished.  It takes
+# a list of candidates and returns the selected node ID with some reasoning.
+ORCHESTRATOR_MODEL = "gpt-4o-2024-11-20"
+
+# Function specification describing the JSON schema for orchestrator output.
+node_selection_spec = FunctionSpec(
+    name="select_best_implementation",
+    description="Select the best implementation based on comprehensive analysis",
+    json_schema={
+        "type": "object",
+        "properties": {
+            "selected_id": {
+                "type": "string",
+                "description": "ID of the selected best implementation",
+            },
+            "reasoning": {
+                "type": "string",
+                "description": "Detailed explanation of why this implementation was chosen",
+            },
+        },
+        "required": ["selected_id", "reasoning"],
+    },
+)
+
+
+class PaperNode:
+    """A paper version on disk (latex_dir contains template.tex)."""
+
+    def __init__(
+        self,
+        latex_dir: Path,
+        depth: int = 0,
+        parent: "PaperNode | None" = None,
+        llm_model: str = DEFAULT_MODEL,
+        vlm_model: str = VLM_MODEL,
+        debug_depth: int = 0,
+    ):
+        self.id = uuid.uuid4().hex
+        self.latex_dir = latex_dir
+        self.depth = depth
+        self.parent = parent
+        self.llm_model = llm_model
+        self.vlm_model = vlm_model
+        self.children: list["PaperNode"] = []
+        self.pdf_path = latex_dir / "template.pdf"  # compiled later
+        self.score: float | None = None
+        self.llm_json: dict | None = None
+        self.vlm_json: dict | None = None
+        # compatibility with treesearch Journal
+        self.is_buggy = False
+        self.is_buggy_plots = False
+        self.debug_depth = debug_depth
+
+    def compile(self):
+        # ``compile_latex`` is reused from the main code base. It simply
+        # invokes ``latexmk`` to build a PDF in ``self.pdf_path``.
+        from ai_scientist.perform_icbinb_writeup import compile_latex  # reuse util
+
+        compile_latex(str(self.latex_dir), str(self.pdf_path))
+
+    def evaluate(self):
+        if not self.pdf_path.exists():
+            self.compile()
+        # Run LLM and VLM reviews then compute an aggregate numeric score.
+        self.llm_json = llm_review(str(self.pdf_path), model=self.llm_model)
+        self.vlm_json = vlm_review(str(self.pdf_path), model=self.vlm_model)
+        self.score = meta_score([self.llm_json, self.vlm_json])
+        # Persist results for analysis
+        with open(self.latex_dir / "reviews.json", "w") as f:
+            json.dump(
+                {"llm": self.llm_json, "vlm": self.vlm_json, "score": self.score},
+                f,
+                indent=2,
+            )
+        return self.score
+
+
+class Journal:
+    """Keep track of explored paper versions."""
+
+    def __init__(self) -> None:
+        self.nodes: list[PaperNode] = []
+
+    def append(self, node: PaperNode) -> None:
+        # ``step`` mirrors the attribute used by the original tree search
+        # implementation and simply records insertion order.
+        node.step = len(self.nodes)
+        self.nodes.append(node)
+
+    def best_node(
+        self, orchestrator_model: str = ORCHESTRATOR_MODEL
+    ) -> PaperNode | None:
+        if not self.nodes:
+            return None
+        if len(self.nodes) == 1:
+            return self.nodes[0]
+
+        # Construct a small system prompt summarising each candidate's score.
+        prompt = {
+            "Introduction": (
+                "You are an experienced researcher choosing the best improved paper version based on review scores."
+            ),
+            "Candidates": "",
+        }
+        for n in self.nodes:
+            prompt["Candidates"] += f"ID: {n.id} Score: {n.score:.3f}\n"
+
+        try:
+            selection = query(
+                system_message=prompt,
+                user_message=None,
+                func_spec=node_selection_spec,
+                model=orchestrator_model,
+                temperature=0.3,
+            )
+            selected = next(
+                (n for n in self.nodes if n.id == selection["selected_id"]), None
+            )
+            if selected:
+                return selected
+        except Exception as exc:
+            # If the orchestrator call fails we simply fall back to a numeric
+            # best-node selection so the search does not crash.
+            logger.error("Orchestrator selection failed: %s", exc)
+        return max(self.nodes, key=lambda n: n.score or 0)
+
+
+def breadth_first_improve(
+    root_dir: Path,
+    seed_ideas: str,
+    human_reviews: str | None = None,
+    *,
+    params: SearchParams | None = None,
+    model_editor: str = EDITOR_MODEL,
+    model_review: str = DEFAULT_MODEL,
+    model_vlm: str = VLM_MODEL,
+    orchestrator_model: str = ORCHESTRATOR_MODEL,
+):
+    """Explore paper edits using a breadth-first expansion order."""
+    p = params or SearchParams(max_depth=3, beam_size=4)
+    # The root node corresponds to the initial paper.  It is evaluated once so
+    # the search has a baseline score.
+    root = PaperNode(root_dir, llm_model=model_review, vlm_model=model_vlm)
+    safe_evaluate(root)
+    journal = Journal()  # track every explored node for later selection
+    journal.append(root)
+    frontier = deque([root])
+    best_state = root
+    # Create a number of draft children before entering the main loop
+    for i in range(p.num_drafts):
+        draft_dir = unique_subdir(root.latex_dir.parent, "draft")
+        shutil.copytree(root.latex_dir, draft_dir)
+        tex_path = draft_dir / "template.tex"
+        new_source = propose_edit(
+            tex_path, seed_ideas, human_reviews, model=model_editor
+        )  # model proposes a complete LaTeX rewrite of template.tex
+        tex_path.write_text(new_source)
+        draft = PaperNode(
+            draft_dir,
+            root.depth + 1,
+            parent=root,
+            llm_model=model_review,
+            vlm_model=model_vlm,
+        )
+        root.children.append(draft)  # keep a tree structure for analysis
+        safe_evaluate(draft)
+        journal.append(draft)
+        frontier.append(draft)
+    # Standard BFS loop
+    # Main priority queue loop
+    while frontier:
+        state = frontier.popleft()
+        logger.info("Evaluating depth=%s dir=%s", state.depth, state.latex_dir)
+        score = state.score if state is root else safe_evaluate(state)
+        if score > best_state.score:
+            best_state = state
+            logger.info("[NEW BEST] score=%.3f at %s", score, state.latex_dir)
+        if state.depth >= p.max_depth:
+            continue
+        # If evaluation failed we may ask the editor model to try again.
+        if (
+            state.is_buggy
+            and state.debug_depth < p.max_debug_depth
+            and random.random() < p.debug_prob
+        ):
+            tex_path = state.latex_dir / "template.tex"
+            new_src = propose_edit(
+                tex_path, seed_ideas, human_reviews, model=model_editor
+            )  # attempt to fix the buggy document
+            tex_path.write_text(new_src)
+            state.debug_depth += 1
+            safe_evaluate(state)
+        # Generate ``beam_size`` children by applying LLM edits to the current
+        # LaTeX source
+        for i in range(p.beam_size):
+            child_dir = unique_subdir(state.latex_dir.parent, f"d{state.depth}")
+            # Copy the current directory so the child starts with the same source
+            shutil.copytree(state.latex_dir, child_dir)
+            tex_path = child_dir / "template.tex"
+            new_source = propose_edit(
+                tex_path,
+                seed_ideas,
+                human_reviews,
+                model=model_editor,
+            )  # LLM suggests an updated LaTeX file
+            # Overwrite the source so subsequent evaluation compiles the new version
+            tex_path.write_text(new_source)
+            child = PaperNode(
+                child_dir,
+                state.depth + 1,
+                parent=state,
+                llm_model=model_review,
+                vlm_model=model_vlm,
+            )
+            state.children.append(child)
+            # Score the newly created child immediately so it can be queued
+            safe_evaluate(child)
+            journal.append(child)
+            frontier.append(child)
+    # Let the orchestrator or fallback logic pick the best candidate overall
+    return journal.best_node(orchestrator_model), journal
+
+
+def tree_search_improve(
+    root_dir: Path,
+    seed_ideas: str,
+    human_reviews: str | None = None,
+    *,
+    params: SearchParams | None = None,
+    model_editor: str = EDITOR_MODEL,
+    model_review: str = DEFAULT_MODEL,
+    model_vlm: str = VLM_MODEL,
+    orchestrator_model: str = ORCHESTRATOR_MODEL,
+):
+    """Priority-based tree search over paper versions."""
+    p = params or SearchParams(max_depth=3, beam_size=4)
+    root = PaperNode(root_dir, llm_model=model_review, vlm_model=model_vlm)
+    safe_evaluate(root)
+    journal = Journal()
+    journal.append(root)
+    frontier: list[tuple[float, PaperNode]] = [(-root.score, root)]
+    # Generate a few initial drafts and push them onto the priority queue
+    for i in range(p.num_drafts):
+        draft_dir = unique_subdir(root.latex_dir.parent, "draft")
+        shutil.copytree(root.latex_dir, draft_dir)
+        tex_path = draft_dir / "template.tex"
+        new_source = propose_edit(
+            tex_path, seed_ideas, human_reviews, model=model_editor
+        )
+        tex_path.write_text(new_source)
+        draft = PaperNode(
+            draft_dir,
+            root.depth + 1,
+            parent=root,
+            llm_model=model_review,
+            vlm_model=model_vlm,
+        )
+        root.children.append(draft)
+        draft.evaluate()
+        journal.append(draft)
+        heapq.heappush(frontier, (-draft.score, draft))  # min-heap by negative score
+
+    # Main priority queue loop
+    while frontier:
+        _, node = heapq.heappop(frontier)
+        logger.info(
+            "Exploring depth=%s dir=%s score=%.3f",
+            node.depth,
+            node.latex_dir,
+            node.score,
+        )
+        if node.depth >= p.max_depth:
+            continue
+        if (
+            node.is_buggy
+            and node.debug_depth < p.max_debug_depth
+            and random.random() < p.debug_prob
+        ):
+            tex_path = node.latex_dir / "template.tex"
+            new_src = propose_edit(
+                tex_path, seed_ideas, human_reviews, model=model_editor
+            )  # attempt recovery
+            tex_path.write_text(new_src)
+            node.debug_depth += 1
+            safe_evaluate(node)
+        # Expand by proposing ``beam_size`` edits from the current node
+        for i in range(p.beam_size):
+            child_dir = unique_subdir(node.latex_dir.parent, f"d{node.depth}")
+            shutil.copytree(node.latex_dir, child_dir)
+            tex_path = child_dir / "template.tex"
+            new_source = propose_edit(
+                tex_path,
+                seed_ideas,
+                human_reviews,
+                model=model_editor,
+            )  # one possible child mutation
+            tex_path.write_text(new_source)
+            child = PaperNode(
+                child_dir,
+                node.depth + 1,
+                parent=node,
+                llm_model=model_review,
+                vlm_model=model_vlm,
+            )
+            node.children.append(child)
+            safe_evaluate(child)
+            journal.append(child)
+            heapq.heappush(frontier, (-child.score, child))  # push onto heap by score
+
+    return journal.best_node(orchestrator_model), journal
diff --git a/ai_scientist/paper_improver/utils.py b/ai_scientist/paper_improver/utils.py
new file mode 100644
index 0000000000000000000000000000000000000000..c67ed0ff6385046a7fd48e14e277be9a0b87c82b
--- /dev/null
+++ b/ai_scientist/paper_improver/utils.py
@@ -0,0 +1,22 @@
+"""Helper utilities for the :mod:`paper_improver` package.
+
+Currently this module only contains ``unique_subdir`` which generates a
+non-colliding directory name.  It is primarily used by the search routines to
+clone LaTeX templates into new working folders.
+"""
+
+from __future__ import annotations
+
+from pathlib import Path
+import uuid
+
+
+def unique_subdir(parent: Path, prefix: str) -> Path:
+    """Return a subdirectory path within *parent* that does not already exist."""
+
+    while True:
+        # Generate a random name and check if it exists.  The directory is not
+        # created here; callers may decide whether to copy or symlink data.
+        cand = parent / f"{prefix}_{uuid.uuid4().hex[:8]}"
+        if not cand.exists():
+            return cand
diff --git a/ai_scientist/paper_improver/vlm_review.py b/ai_scientist/paper_improver/vlm_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..c5a6129a0ca75616c36409f5b6c0fe86c1b69359
--- /dev/null
+++ b/ai_scientist/paper_improver/vlm_review.py
@@ -0,0 +1,20 @@
+"""Thin wrapper around :func:`perform_imgs_cap_ref_review`.
+
+The :mod:`perform_vlm_review` module contains the heavy logic for assessing a
+paper's figures and captions with a vision-language model.  Here we expose a
+single ``vlm_review`` function that selects the model and passes through the
+PDF path.  This mirrors ``llm_review`` for textual reviews.
+"""
+from ai_scientist.perform_vlm_review import perform_imgs_cap_ref_review
+from ai_scientist.vlm import create_client as create_vlm_client
+
+VLM_MODEL = "gpt-4o-2024-11-20"
+
+
+def vlm_review(pdf_path: str, *, model: str = VLM_MODEL) -> dict:
+    """Run the vision-language review over a compiled PDF."""
+    client, m = create_vlm_client(model)
+    # Delegates to ``perform_imgs_cap_ref_review`` which expects a client and
+    # model name.  The function returns parsed JSON describing any detected
+    # figure issues.
+    return perform_imgs_cap_ref_review(client, m, pdf_path)
diff --git a/examples/paper_improver_minimal/README.md b/examples/paper_improver_minimal/README.md
new file mode 100644
index 0000000000000000000000000000000000000000..34c1d5e720822d181a23c0661791b7ac8bfe2548
--- /dev/null
+++ b/examples/paper_improver_minimal/README.md
@@ -0,0 +1,21 @@
+# Minimal Paper Improver Example
+
+This directory contains a tiny LaTeX project with a placeholder figure, a
+bibliography, seed improvement ideas and reviewer comments. It can be used to
+try the `paper_improver` pipeline without extra setup.
+
+Example command:
+
+```bash
+python scripts/launch_paper_improver.py examples/paper_improver_minimal \
+    examples/paper_improver_minimal/seed_ideas.json \
+    --human-reviews examples/paper_improver_minimal/human_reviews.txt \
+    --max-depth 1 --beam-size 1 --num-drafts 1 \
+    --debug-prob 0.5 --max-debug-depth 3 \
+    --model-editor o1-preview-2024-09-12 \
+    --model-review gpt-4o-2024-11-20 \
+    --model-vlm gpt-4o-2024-11-20 \
+    --model-orchestrator gpt-4o-2024-11-20 \
+    --model-citation gpt-4o-2024-11-20 --num-cite-rounds 20 \
+    --model-reflection o1-preview-2024-09-12 --num-reflections 1 --page-limit 4
+```
diff --git a/examples/paper_improver_minimal/human_reviews.txt b/examples/paper_improver_minimal/human_reviews.txt
new file mode 100644
index 0000000000000000000000000000000000000000..2cbbd116a12611be7213b8d287759a4fbb388b79
--- /dev/null
+++ b/examples/paper_improver_minimal/human_reviews.txt
@@ -0,0 +1,4 @@
+- The introduction lacks sufficient background on automated writing systems.
+- Please cite additional related work.
+- Explain the purpose of Figure 1 in more detail.
+- Conclude with a clearer summary of contributions.
diff --git a/examples/paper_improver_minimal/references.bib b/examples/paper_improver_minimal/references.bib
new file mode 100644
index 0000000000000000000000000000000000000000..7e47bb77c2840ebbb866c6a53cc47caaa9f5e774
--- /dev/null
+++ b/examples/paper_improver_minimal/references.bib
@@ -0,0 +1,13 @@
+@article{test,
+  title={Minimal example reference},
+  author={Author, A.},
+  journal={Nowhere},
+  year={2024}
+}
+
+@article{related,
+  title={Related work},
+  author={Researcher, B.},
+  journal={Some Journal},
+  year={2023}
+}
diff --git a/examples/paper_improver_minimal/seed_ideas.json b/examples/paper_improver_minimal/seed_ideas.json
new file mode 100644
index 0000000000000000000000000000000000000000..9427748b27c4a0fa0592bad484fbe07611c3f48f
--- /dev/null
+++ b/examples/paper_improver_minimal/seed_ideas.json
@@ -0,0 +1,8 @@
+{
+  "ideas": [
+    "Expand the introduction with background and related work",
+    "Include a table of results",
+    "Add a workflow diagram",
+    "Clarify the conclusion"
+  ]
+}
diff --git a/examples/paper_improver_minimal/template.tex b/examples/paper_improver_minimal/template.tex
new file mode 100644
index 0000000000000000000000000000000000000000..ef34ad06ef1bb41d2a0501607027626c917e3ee4
--- /dev/null
+++ b/examples/paper_improver_minimal/template.tex
@@ -0,0 +1,17 @@
+\documentclass{article}
+\usepackage{graphicx}
+\usepackage{mwe}
+\begin{document}
+\section{Introduction}
+This short paper demonstrates the paper improver pipeline. Figure~\ref{fig:example} shows a placeholder image. We follow the approach of~\cite{test}.
+
+\begin{figure}
+  \centering
+  \includegraphics[width=0.5\linewidth]{example-image}
+  \caption{Example figure}
+  \label{fig:example}
+\end{figure}
+
+\bibliographystyle{plain}
+\bibliography{references}
+\end{document}
diff --git a/scripts/launch_paper_improver.py b/scripts/launch_paper_improver.py
new file mode 100755
index 0000000000000000000000000000000000000000..e7fedec025662b2a98d86b375185800c8b4f860f
--- /dev/null
+++ b/scripts/launch_paper_improver.py
@@ -0,0 +1,123 @@
+#!/usr/bin/env python3
+"""Command line entry point for ``paper_improver``.
+
+This script parses command line arguments, configures logging and environment
+variables and then invokes :func:`ai_scientist.paper_improver.improve_paper`.
+It mirrors the ``launch_scientist_bfts.py`` script from the main project but
+omits experiment execution.
+"""
+import argparse
+import json
+import logging
+from pathlib import Path
+from ai_scientist.paper_improver import improve_paper
+
+parser = argparse.ArgumentParser(
+    description="Iteratively improve an existing paper via AI-Scientist pipeline"
+)
+parser.add_argument("latex_dir", help="Directory containing template.tex (and figures)")
+parser.add_argument(
+    "seed_ideas_json", help="JSON file with high-level improvement ideas"
+)
+parser.add_argument(
+    "--human-reviews", help="Path to txt/markdown file with reviewer comments"
+)
+parser.add_argument("--max-depth", type=int, default=2)
+parser.add_argument("--beam-size", type=int, default=3)
+parser.add_argument("--num-drafts", type=int, default=3)
+parser.add_argument("--debug-prob", type=float, default=0.5)
+parser.add_argument("--max-debug-depth", type=int, default=3)
+parser.add_argument(
+    "--strategy",
+    choices=["bfs", "tree"],
+    default="bfs",
+    help="Search strategy to use: simple bfs or priority tree search",
+)
+parser.add_argument(
+    "--model-editor",
+    default="o1-preview-2024-09-12",
+    help="LLM used to propose edits",
+)
+parser.add_argument(
+    "--model-review",
+    default="gpt-4o-2024-11-20",
+    help="Model used for text-based review",
+)
+parser.add_argument(
+    "--model-vlm",
+    default="gpt-4o-2024-11-20",
+    help="Model used for VLM figure review",
+)
+parser.add_argument(
+    "--model-orchestrator",
+    default="gpt-4o-2024-11-20",
+    help="Model used to select the best node",
+)
+parser.add_argument(
+    "--model-citation",
+    default="gpt-4o-2024-11-20",
+    help="Model used for citation gathering",
+)
+parser.add_argument(
+    "--num-cite-rounds",
+    type=int,
+    default=20,
+    help="Number of citation rounds",
+)
+parser.add_argument(
+    "--model-reflection",
+    default="o1-preview-2024-09-12",
+    help="Model used for final reflection",
+)
+parser.add_argument(
+    "--num-reflections",
+    type=int,
+    default=3,
+    help="Number of reflection steps",
+)
+parser.add_argument(
+    "--page-limit",
+    type=int,
+    default=4,
+    help="Page limit for final reflection",
+)
+parser.add_argument("--openai-api-key")
+parser.add_argument("--gemini-api-key")
+
+args = parser.parse_args()
+
+logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
+
+# Allow API keys to be supplied via command line to avoid modifying the shell
+if args.openai_api_key:
+    import os
+
+    os.environ["OPENAI_API_KEY"] = args.openai_api_key
+if args.gemini_api_key:
+    import os
+
+    os.environ["GEMINI_API_KEY"] = args.gemini_api_key
+
+seed_ideas = Path(args.seed_ideas_json).read_text()
+human_reviews = Path(args.human_reviews).read_text() if args.human_reviews else None
+
+improve_paper(
+    args.latex_dir,
+    seed_ideas,
+    human_reviews,
+    max_depth=args.max_depth,
+    beam_size=args.beam_size,
+    num_drafts=args.num_drafts,
+    debug_prob=args.debug_prob,
+    max_debug_depth=args.max_debug_depth,
+    strategy=args.strategy,
+    model_editor=args.model_editor,
+    model_review=args.model_review,
+    model_vlm=args.model_vlm,
+    orchestrator_model=args.model_orchestrator,
+    model_citation=args.model_citation,
+    num_cite_rounds=args.num_cite_rounds,
+    model_reflection=args.model_reflection,
+    num_reflections=args.num_reflections,
+    page_limit=args.page_limit,
+)  # run the search pipeline
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000000000000000000000000000000000000..a9ed89e78e6ae4d05a9e63d3ab7e31e91c1c8dbd
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,59 @@
+import sys
+from pathlib import Path
+
+# Add project root to path
+root = Path(__file__).resolve().parents[1]
+sys.path.insert(0, str(root))
+
+# Provide a stub for the treesearch backend to avoid heavy dependencies
+import types
+
+class DummySpec:
+    def __init__(self, *args, **kwargs):
+        pass
+
+stub_backend = types.SimpleNamespace(
+    query=lambda **kwargs: {"selected_id": "", "reasoning": ""},
+    FunctionSpec=DummySpec,
+)
+sys.modules.setdefault("ai_scientist.treesearch.backend", stub_backend)
+
+# Stub out heavy LLM dependencies
+token_stub = types.SimpleNamespace(track_token_usage=lambda f: f)
+sys.modules.setdefault("ai_scientist.utils.token_tracker", token_stub)
+
+class DummyClient:
+    class chat:
+        class completions:
+            @staticmethod
+            def create(**kwargs):
+                return types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""))])
+
+def create_client(model):
+    return DummyClient(), model
+
+llm_stub = types.SimpleNamespace(create_client=create_client)
+sys.modules.setdefault("ai_scientist.llm", llm_stub)
+
+sys.modules.setdefault("tiktoken", types.ModuleType("tiktoken"))
+
+sys.modules.setdefault("ai_scientist.perform_llm_review", types.SimpleNamespace(perform_review=lambda *a, **k: {}))
+sys.modules.setdefault(
+    "ai_scientist.perform_vlm_review",
+    types.SimpleNamespace(
+        perform_imgs_cap_ref_review=lambda *a, **k: {},
+        detect_duplicate_figures=lambda *a, **k: {},
+    ),
+)
+sys.modules.setdefault(
+    "ai_scientist.perform_icbinb_writeup",
+    types.SimpleNamespace(
+        gather_citations=lambda *a, **k: None,
+        compile_latex=lambda cwd, pdf: Path(pdf).write_text(""),
+        get_reflection_page_info=lambda pdf, limit: "",
+    ),
+)
+sys.modules.setdefault("backoff", types.ModuleType("backoff"))
+sys.modules.setdefault("openai", types.ModuleType("openai"))
+vlm_stub = types.SimpleNamespace(create_client=create_client)
+sys.modules.setdefault("ai_scientist.vlm", vlm_stub)
diff --git a/tests/test_cli.py b/tests/test_cli.py
new file mode 100644
index 0000000000000000000000000000000000000000..850ed1b2ca66a46e60d273cd35f5721368669842
--- /dev/null
+++ b/tests/test_cli.py
@@ -0,0 +1,88 @@
+import sys
+import runpy
+from pathlib import Path
+
+def test_cli_parsing(monkeypatch, tmp_path):
+    project = tmp_path / "p"
+    project.mkdir()
+    (project / "template.tex").write_text("t")
+    ideas = tmp_path / "ideas.json"
+    ideas.write_text("{}")
+    reviews = tmp_path / "rev.txt"
+    reviews.write_text("r")
+
+    captured = {}
+
+    def fake_improve(latex_dir, seed_ideas, human_reviews=None, **kwargs):
+        captured["latex_dir"] = Path(latex_dir)
+        captured["seed_ideas"] = seed_ideas
+        captured["human_reviews"] = human_reviews
+        captured["kwargs"] = kwargs
+
+    monkeypatch.setattr(
+        "ai_scientist.paper_improver.pipeline.improve_paper", fake_improve
+    )
+    # __init__ re-exports improve_paper; patch it too so the CLI uses our stub
+    monkeypatch.setattr(
+        "ai_scientist.paper_improver.improve_paper", fake_improve
+    )
+
+    argv = [
+        "prog",
+        str(project),
+        str(ideas),
+        "--human-reviews",
+        str(reviews),
+        "--max-depth",
+        "1",
+        "--beam-size",
+        "2",
+        "--num-drafts",
+        "3",
+        "--debug-prob",
+        "0.3",
+        "--max-debug-depth",
+        "4",
+        "--strategy",
+        "tree",
+        "--model-editor",
+        "E",
+        "--model-review",
+        "R",
+        "--model-vlm",
+        "V",
+        "--model-orchestrator",
+        "O",
+        "--model-citation",
+        "C",
+        "--num-cite-rounds",
+        "5",
+        "--model-reflection",
+        "M",
+        "--num-reflections",
+        "2",
+        "--page-limit",
+        "7",
+    ]
+    monkeypatch.setattr(sys, "argv", argv)
+    runpy.run_path("scripts/launch_paper_improver.py", run_name="__main__")
+
+    assert captured["latex_dir"] == project
+    assert captured["seed_ideas"] == ideas.read_text()
+    assert captured["human_reviews"] == reviews.read_text()
+    kw = captured["kwargs"]
+    assert kw["max_depth"] == 1
+    assert kw["beam_size"] == 2
+    assert kw["num_drafts"] == 3
+    assert kw["debug_prob"] == 0.3
+    assert kw["max_debug_depth"] == 4
+    assert kw["strategy"] == "tree"
+    assert kw["model_editor"] == "E"
+    assert kw["model_review"] == "R"
+    assert kw["model_vlm"] == "V"
+    assert kw["orchestrator_model"] == "O"
+    assert kw["model_citation"] == "C"
+    assert kw["num_cite_rounds"] == 5
+    assert kw["model_reflection"] == "M"
+    assert kw["num_reflections"] == 2
+    assert kw["page_limit"] == 7
diff --git a/tests/test_extra.py b/tests/test_extra.py
new file mode 100644
index 0000000000000000000000000000000000000000..1b5cc9090faf66cf426b9f642df4515b7ba2ea1e
--- /dev/null
+++ b/tests/test_extra.py
@@ -0,0 +1,115 @@
+import types
+from pathlib import Path
+import pytest
+
+from ai_scientist.paper_improver import search, reflection, meta_review
+
+
+def test_safe_evaluate_marks_buggy(monkeypatch, tmp_path):
+    node = search.PaperNode(tmp_path)
+
+    def bad_eval(self):
+        raise RuntimeError("boom")
+
+    monkeypatch.setattr(search.PaperNode, "evaluate", bad_eval)
+    res = search.safe_evaluate(node)
+    assert res is None
+    assert node.is_buggy
+
+
+def test_journal_best_node_fallback(monkeypatch):
+    j = search.Journal()
+    n1 = search.PaperNode(Path("p1"))
+    n2 = search.PaperNode(Path("p2"))
+    n1.score = 1.0
+    n2.score = 2.0
+    j.append(n1)
+    j.append(n2)
+
+    def boom(**kw):
+        raise RuntimeError("fail")
+
+    monkeypatch.setattr(search, "query", boom)
+    best = j.best_node("model")
+    assert best is n2  # falls back to numeric max
+
+
+def test_meta_score_missing_keys():
+    review = {"Overall": 6}
+    score = meta_review.score_single(review)
+    assert score > 0
+    assert meta_review.meta_score([review]) == pytest.approx(score)
+
+
+def test_reflect_paper_early_exit(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    tex = root / "template.tex"
+    tex.write_text("A")
+
+    calls = {"compile": 0}
+
+    def fake_compile(cwd, pdf):
+        calls["compile"] += 1
+        Path(pdf).write_text("pdf")
+
+    class DummyClient:
+        class chat:
+            class completions:
+                @staticmethod
+                def create(**kw):
+                    return types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content="```latex\nA\n```"))])
+
+    monkeypatch.setattr(reflection, "compile_latex", fake_compile)
+    monkeypatch.setattr(reflection, "create_client", lambda m: (DummyClient(), m))
+    monkeypatch.setattr(reflection, "create_vlm_client", lambda m: (object(), m))
+    monkeypatch.setattr(reflection, "perform_imgs_cap_ref_review", lambda *a, **k: {})
+    monkeypatch.setattr(reflection, "detect_duplicate_figures", lambda *a, **k: {})
+    monkeypatch.setattr(reflection, "get_reflection_page_info", lambda *a, **k: "")
+    monkeypatch.setattr(reflection.subprocess, "run", lambda *a, **k: types.SimpleNamespace(stdout=""))
+
+    reflection.reflect_paper(root, num_rounds=3, model="m", vlm_model="v")
+    # compile should be called once in loop and once at end -> 2
+    assert calls["compile"] == 2
+
+
+def test_cli_defaults(monkeypatch, tmp_path):
+    project = tmp_path / "p"
+    project.mkdir()
+    (project / "template.tex").write_text("T")
+    ideas = tmp_path / "ideas.json"
+    ideas.write_text("{}")
+
+    captured = {}
+
+    def fake_improve(latex_dir, seed_ideas, human_reviews=None, **kwargs):
+        captured["kwargs"] = kwargs
+        captured["latex_dir"] = Path(latex_dir)
+        captured["seed_ideas"] = seed_ideas
+        captured["human_reviews"] = human_reviews
+
+    monkeypatch.setattr("ai_scientist.paper_improver.pipeline.improve_paper", fake_improve)
+    monkeypatch.setattr("ai_scientist.paper_improver.improve_paper", fake_improve)
+
+    argv = ["prog", str(project), str(ideas)]
+    monkeypatch.setattr(__import__("sys"), "argv", argv)
+    import runpy
+
+    runpy.run_path("scripts/launch_paper_improver.py", run_name="__main__")
+
+    kw = captured["kwargs"]
+    assert kw["strategy"] == "bfs"
+    assert kw["max_depth"] == 2
+    assert kw["beam_size"] == 3
+    assert kw["num_drafts"] == 3
+    assert kw["debug_prob"] == 0.5
+    assert kw["max_debug_depth"] == 3
+    assert kw["model_editor"]
+    assert kw["model_review"]
+    assert kw["model_vlm"]
+    assert kw["orchestrator_model"]
+    assert kw["model_citation"]
+    assert kw["num_cite_rounds"] == 20
+    assert kw["model_reflection"]
+    assert kw["num_reflections"] == 3
+    assert kw["page_limit"] == 4
diff --git a/tests/test_latex_editor.py b/tests/test_latex_editor.py
new file mode 100644
index 0000000000000000000000000000000000000000..5210e5c15c1749bb2808002848ad8b504360a2a2
--- /dev/null
+++ b/tests/test_latex_editor.py
@@ -0,0 +1,23 @@
+from pathlib import Path
+import types
+from ai_scientist.paper_improver import latex_editor
+
+
+def test_propose_edit_extract(monkeypatch, tmp_path):
+    tex = tmp_path / "t.tex"
+    tex.write_text("Original")
+
+    class DummyCompletions:
+        @staticmethod
+        def create(**kwargs):
+            return types.SimpleNamespace(
+                choices=[types.SimpleNamespace(message=types.SimpleNamespace(content="```latex\nEDITED\n```"))]
+            )
+
+    class DummyClient:
+        chat = types.SimpleNamespace(completions=DummyCompletions)
+
+    monkeypatch.setattr(latex_editor, "create_client", lambda model: (DummyClient(), model))
+
+    result = latex_editor.propose_edit(tex, "idea", model="dummy")
+    assert result == "EDITED"
diff --git a/tests/test_massive.py b/tests/test_massive.py
new file mode 100644
index 0000000000000000000000000000000000000000..4cc1195d5551f496832cec12e9b70980379b7772
--- /dev/null
+++ b/tests/test_massive.py
@@ -0,0 +1,38 @@
+import pytest
+from ai_scientist.paper_improver import meta_review, utils, search
+
+
+@pytest.mark.parametrize("overall", range(1, 51))
+def test_score_single_range(overall):
+    review = {
+        "Originality": 1,
+        "Quality": 1,
+        "Clarity": 1,
+        "Significance": 1,
+        "Overall": overall,
+    }
+    score = meta_review.score_single(review)
+    assert score > 0
+
+
+@pytest.mark.parametrize("prefix", [f"p{i}" for i in range(25)])
+def test_unique_subdir_multiple(tmp_path, prefix):
+    paths = {utils.unique_subdir(tmp_path, prefix) for _ in range(4)}
+    assert len(paths) == 4
+    assert all(not p.exists() and p.parent == tmp_path for p in paths)
+
+
+@pytest.mark.parametrize("idx", range(25))
+def test_searchparams_fields(idx):
+    p = search.SearchParams(
+        max_depth=idx + 1,
+        beam_size=idx + 2,
+        num_drafts=idx % 3,
+        debug_prob=0.1,
+        max_debug_depth=idx + 3,
+    )
+    assert p.max_depth == idx + 1
+    assert p.beam_size == idx + 2
+    assert p.num_drafts == idx % 3
+    assert p.debug_prob == 0.1
+    assert p.max_debug_depth == idx + 3
diff --git a/tests/test_meta_review.py b/tests/test_meta_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..a123e8c7a437397038c01ce5f65cb4e6e0f1543d
--- /dev/null
+++ b/tests/test_meta_review.py
@@ -0,0 +1,21 @@
+import math
+from ai_scientist.paper_improver.meta_review import score_single, meta_score
+
+
+def test_score_single():
+    review = {
+        "Originality": 2,
+        "Quality": 3,
+        "Clarity": 3,
+        "Significance": 4,
+        "Overall": 8,
+    }
+    expected = (3*2 + 3*3 + 2*3 + 4*4 + 5*(8/2.5)) / 17
+    assert math.isclose(score_single(review), expected)
+
+
+def test_meta_score_average():
+    r1 = {"Originality": 1, "Quality": 1, "Clarity": 1, "Significance": 1, "Overall": 5}
+    r2 = {"Originality": 2, "Quality": 2, "Clarity": 2, "Significance": 2, "Overall": 6}
+    expected = (score_single(r1) + score_single(r2)) / 2
+    assert math.isclose(meta_score([r1, r2]), expected)
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..052797d887b3cad8a101839786d01c0f2e7ae71c
--- /dev/null
+++ b/tests/test_pipeline.py
@@ -0,0 +1,253 @@
+from pathlib import Path
+from ai_scientist.paper_improver import pipeline
+
+
+def test_improve_paper_strategy(monkeypatch, tmp_path):
+    called = {}
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("test")
+
+    def fake_bfs(*args, **kwargs):
+        called['bfs'] = True
+        class R:
+            latex_dir = Path('bfs')
+        return R(), None
+
+    def fake_tree(*args, **kwargs):
+        called['tree'] = True
+        class R:
+            latex_dir = Path('tree')
+        return R(), None
+
+    monkeypatch.setattr(pipeline, 'breadth_first_improve', fake_bfs)
+    monkeypatch.setattr(pipeline, 'tree_search_improve', fake_tree)
+    monkeypatch.setattr(pipeline, 'reflect_paper', lambda *a, **k: None)
+
+    pipeline.improve_paper(root, 'ideas', strategy='bfs')
+    pipeline.improve_paper(root, 'ideas', strategy='tree')
+
+    assert called == {'bfs': True, 'tree': True}
+
+
+def test_improve_paper_citations(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("tex")
+
+    def fake_bfs(*args, **kwargs):
+        class R:
+            latex_dir = root
+        return R(), None
+
+    captured = {}
+
+    def fake_gather(path, num_cite_rounds=20, small_model="m"):
+        captured["args"] = (Path(path), num_cite_rounds, small_model)
+
+    monkeypatch.setattr(pipeline, "breadth_first_improve", fake_bfs)
+    monkeypatch.setattr(pipeline, "gather_citations", fake_gather)
+    monkeypatch.setattr(pipeline, "reflect_paper", lambda *a, **k: None)
+
+    pipeline.improve_paper(
+        root,
+        "ideas",
+        model_citation="cmodel",
+        num_cite_rounds=3,
+    )
+
+    assert captured["args"] == (root.resolve(), 3, "cmodel")
+
+
+def test_improve_paper_hyperparams_bfs(monkeypatch, tmp_path):
+    root = tmp_path / "p"
+    root.mkdir()
+    (root / "template.tex").write_text("t")
+
+    captured = {}
+
+    def fake_bfs(
+        root_dir,
+        seed_ideas,
+        human_reviews,
+        *,
+        params,
+        model_editor,
+        model_review,
+        model_vlm,
+        orchestrator_model,
+        **kwargs,
+    ):
+        captured["root_dir"] = Path(root_dir)
+        captured["params"] = params
+        captured["editor"] = model_editor
+        captured["review"] = model_review
+        captured["vlm"] = model_vlm
+        captured["orch"] = orchestrator_model
+        class R:
+            latex_dir = root_dir
+        return R(), None
+
+    monkeypatch.setattr(pipeline, "breadth_first_improve", fake_bfs)
+    monkeypatch.setattr(pipeline, "gather_citations", lambda *a, **k: None)
+    monkeypatch.setattr(pipeline, "reflect_paper", lambda *a, **k: None)
+
+    pipeline.improve_paper(
+        root,
+        "ideas",
+        strategy="bfs",
+        model_editor="e",
+        model_review="r",
+        model_vlm="v",
+        orchestrator_model="o",
+        max_depth=5,
+        beam_size=2,
+        num_drafts=1,
+        debug_prob=0.1,
+        max_debug_depth=4,
+        num_cite_rounds=0,
+    )
+
+    p = captured["params"]
+    assert p.max_depth == 5
+    assert p.beam_size == 2
+    assert p.num_drafts == 1
+    assert p.debug_prob == 0.1
+    assert p.max_debug_depth == 4
+    assert captured["editor"] == "e"
+    assert captured["review"] == "r"
+    assert captured["vlm"] == "v"
+    assert captured["orch"] == "o"
+
+
+def test_improve_paper_hyperparams_tree(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("t")
+
+    captured = {}
+
+    def fake_tree(
+        root_dir,
+        seed_ideas,
+        human_reviews,
+        *,
+        params,
+        model_editor,
+        model_review,
+        model_vlm,
+        orchestrator_model,
+        **kwargs,
+    ):
+        captured["root_dir"] = Path(root_dir)
+        captured["params"] = params
+        captured["editor"] = model_editor
+        captured["review"] = model_review
+        captured["vlm"] = model_vlm
+        captured["orch"] = orchestrator_model
+        class R:
+            latex_dir = root_dir
+        return R(), None
+
+    monkeypatch.setattr(pipeline, "tree_search_improve", fake_tree)
+    monkeypatch.setattr(pipeline, "gather_citations", lambda *a, **k: None)
+    monkeypatch.setattr(pipeline, "reflect_paper", lambda *a, **k: None)
+
+    pipeline.improve_paper(
+        root,
+        "ideas",
+        strategy="tree",
+        model_editor="e2",
+        model_review="r2",
+        model_vlm="v2",
+        orchestrator_model="o2",
+        max_depth=4,
+        beam_size=5,
+        num_drafts=2,
+        debug_prob=0.2,
+        max_debug_depth=5,
+        num_cite_rounds=0,
+    )
+
+    p = captured["params"]
+    assert p.max_depth == 4
+    assert p.beam_size == 5
+    assert p.num_drafts == 2
+    assert p.debug_prob == 0.2
+    assert p.max_debug_depth == 5
+    assert captured["editor"] == "e2"
+    assert captured["review"] == "r2"
+    assert captured["vlm"] == "v2"
+    assert captured["orch"] == "o2"
+
+
+def test_citation_defaults(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("t")
+
+    def fake_bfs(*args, **kwargs):
+        class R:
+            latex_dir = root
+        return R(), None
+
+    captured = {}
+
+    def fake_gather(path, num_cite_rounds=20, small_model="m"):
+        captured["args"] = (Path(path), num_cite_rounds, small_model)
+
+    monkeypatch.setattr(pipeline, "breadth_first_improve", fake_bfs)
+    monkeypatch.setattr(pipeline, "gather_citations", fake_gather)
+
+    pipeline.improve_paper(root, "ideas", num_cite_rounds=2)
+
+    assert captured["args"] == (root.resolve(), 2, pipeline.CITATION_MODEL)
+
+
+
+def test_reflection_params(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("t")
+
+    def fake_bfs(*args, **kwargs):
+        class R:
+            latex_dir = root
+        return R(), None
+
+    captured = {}
+
+    def fake_reflect(path, *, model, vlm_model, num_rounds, page_limit):
+        captured["args"] = (Path(path), model, vlm_model, num_rounds, page_limit)
+
+    monkeypatch.setattr(pipeline, "breadth_first_improve", fake_bfs)
+    monkeypatch.setattr(pipeline, "gather_citations", lambda *a, **k: None)
+    monkeypatch.setattr(pipeline, "reflect_paper", fake_reflect)
+
+    pipeline.improve_paper(root, "ideas", num_cite_rounds=0)
+
+    assert captured["args"] == (
+        root.resolve(),
+        pipeline.REFLECTION_MODEL_DEFAULT,
+        pipeline.VLM_MODEL,
+        pipeline.DEFAULT_REFLECTION_ROUNDS,
+        pipeline.DEFAULT_PAGE_LIMIT_VALUE,
+    )
+
+    captured.clear()
+    pipeline.improve_paper(
+        root,
+        "ideas",
+        model_reflection="m",
+        num_reflections=2,
+        page_limit=5,
+        num_cite_rounds=0,
+    )
+
+    assert captured["args"] == (
+        root.resolve(),
+        "m",
+        pipeline.VLM_MODEL,
+        2,
+        5,
+    )
diff --git a/tests/test_reflection.py b/tests/test_reflection.py
new file mode 100644
index 0000000000000000000000000000000000000000..a6765c453990b54e9df2dc29c1de6b97246552a5
--- /dev/null
+++ b/tests/test_reflection.py
@@ -0,0 +1,33 @@
+from pathlib import Path
+import types
+from ai_scientist.paper_improver import reflection
+
+
+def test_reflect_paper(monkeypatch, tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    tex = root / "template.tex"
+    tex.write_text("A")
+
+    def fake_compile(cwd, pdf):
+        Path(pdf).write_text(Path(cwd).joinpath("template.tex").read_text())
+
+    class DummyClient:
+        class chat:
+            class completions:
+                @staticmethod
+                def create(**kw):
+                    return types.SimpleNamespace(
+                        choices=[types.SimpleNamespace(message=types.SimpleNamespace(content="```latex\n" + tex.read_text() + "B\n```"))]
+                    )
+
+    monkeypatch.setattr(reflection, "compile_latex", fake_compile)
+    monkeypatch.setattr(reflection, "create_client", lambda m: (DummyClient(), m))
+    monkeypatch.setattr(reflection, "create_vlm_client", lambda m: (object(), m))
+    monkeypatch.setattr(reflection, "perform_imgs_cap_ref_review", lambda *a, **k: {})
+    monkeypatch.setattr(reflection, "detect_duplicate_figures", lambda *a, **k: {})
+    monkeypatch.setattr(reflection, "get_reflection_page_info", lambda *a, **k: "")
+    monkeypatch.setattr(reflection.subprocess, "run", lambda *a, **k: types.SimpleNamespace(stdout=""))
+
+    reflection.reflect_paper(root, num_rounds=1, model="m", vlm_model="v", page_limit=4)
+    assert tex.read_text().endswith("B")
diff --git a/tests/test_search.py b/tests/test_search.py
new file mode 100644
index 0000000000000000000000000000000000000000..e651488e373a0b36abd9688a7f22b443fbc7d612
--- /dev/null
+++ b/tests/test_search.py
@@ -0,0 +1,164 @@
+from pathlib import Path
+from ai_scientist.paper_improver import search
+
+
+def setup_dummy_env(tmp_path):
+    root = tmp_path / "paper"
+    root.mkdir()
+    (root / "template.tex").write_text("Start")
+    return root
+
+
+def fake_compile(cwd, pdf_file):
+    src = Path(cwd) / "template.tex"
+    Path(pdf_file).write_text(src.read_text())
+
+
+def fake_propose_edit(path, seed_ideas, human_reviews, model=None):
+    return path.read_text() + "X"
+
+
+def fake_review(pdf_path):
+    text = Path(pdf_path).read_text()
+    overall = min(10, len(text))
+    return {
+        "Originality": 1,
+        "Quality": 1,
+        "Clarity": 1,
+        "Significance": 1,
+        "Overall": overall,
+    }
+
+
+class DummyJournal(search.Journal):
+    # override best_node to avoid LLM call
+    def best_node(self, orchestrator_model=search.ORCHESTRATOR_MODEL):
+        return max(self.nodes, key=lambda n: n.score or 0)
+
+
+def run_search(impl, tmp_path, monkeypatch):
+    root = setup_dummy_env(tmp_path)
+
+    counter = {"i": 0}
+
+    def patched_evaluate(self):
+        counter["i"] += 1
+        self.score = float(counter["i"])
+        return self.score
+
+    monkeypatch.setattr(search.PaperNode, "evaluate", patched_evaluate)
+    monkeypatch.setattr(
+        search.PaperNode,
+        "compile",
+        lambda self: fake_compile(self.latex_dir, self.pdf_path),
+    )
+    monkeypatch.setattr(search, "propose_edit", fake_propose_edit)
+    monkeypatch.setattr(search, "llm_review", fake_review)
+    monkeypatch.setattr(search, "vlm_review", fake_review)
+    monkeypatch.setattr(search, "Journal", DummyJournal)
+    params = search.SearchParams(max_depth=2, beam_size=1, num_drafts=0)
+    best, journal = impl(root, "ideas", None, params=params)
+    for n in journal.nodes:
+        print("NODE", n.depth, n.score)
+        assert n.score is not None
+    return best, journal
+
+
+def test_bfs_and_tree_search(monkeypatch, tmp_path):
+    best_bfs, _ = run_search(search.breadth_first_improve, tmp_path, monkeypatch)
+    assert best_bfs.depth == 2
+
+    tmp2 = tmp_path / "other"
+    tmp2.mkdir()
+    best_tree, _ = run_search(search.tree_search_improve, tmp2, monkeypatch)
+    assert best_tree.depth == 2
+
+
+def test_search_params_usage(monkeypatch, tmp_path):
+    root = setup_dummy_env(tmp_path)
+    propose_calls = []
+
+    def patched_propose(path, seed_ideas, human_reviews, model=None):
+        propose_calls.append(path)
+        return path.read_text() + "X"
+
+    counter = {"i": 0}
+
+    def patched_evaluate(self):
+        counter["i"] += 1
+        self.score = float(counter["i"])
+        return self.score
+
+    monkeypatch.setattr(search, "propose_edit", patched_propose)
+    monkeypatch.setattr(search.PaperNode, "evaluate", patched_evaluate)
+    monkeypatch.setattr(search.PaperNode, "compile", lambda self: fake_compile(self.latex_dir, self.pdf_path))
+    monkeypatch.setattr(search, "llm_review", fake_review)
+    monkeypatch.setattr(search, "vlm_review", fake_review)
+    monkeypatch.setattr(search, "Journal", DummyJournal)
+
+    params = search.SearchParams(max_depth=1, beam_size=2, num_drafts=3)
+    best, journal = search.breadth_first_improve(root, "ideas", None, params=params)
+
+    assert len(propose_calls) == 5  # 3 drafts + 2 expansions
+    assert max(n.depth for n in journal.nodes) == 1
+    assert len(journal.nodes) == 6
+
+    tmp2 = tmp_path / "tree"
+    tmp2.mkdir()
+    propose_calls.clear()
+    root2 = setup_dummy_env(tmp2)
+    best2, journal2 = search.tree_search_improve(root2, "ideas", None, params=params)
+
+    assert len(propose_calls) == 5
+    assert max(n.depth for n in journal2.nodes) == 1
+    assert len(journal2.nodes) == 6
+
+
+def test_debug_retry(monkeypatch, tmp_path):
+    root = setup_dummy_env(tmp_path)
+    calls = []
+
+    def failing_eval(self):
+        calls.append("eval")
+        if len(calls) == 1:
+            self.score = 0
+            raise RuntimeError("fail")
+        self.score = 1
+        return 1
+
+    monkeypatch.setattr(search.PaperNode, "evaluate", failing_eval)
+    monkeypatch.setattr(search, "propose_edit", lambda *a, **k: "X")
+    monkeypatch.setattr(search.PaperNode, "compile", lambda self: None)
+    monkeypatch.setattr(search, "llm_review", lambda *a, **k: {})
+    monkeypatch.setattr(search, "vlm_review", lambda *a, **k: {})
+    monkeypatch.setattr(search, "Journal", DummyJournal)
+    monkeypatch.setattr(search.random, "random", lambda: 0.0)
+
+    params = search.SearchParams(max_depth=1, beam_size=0, num_drafts=0, debug_prob=1.0, max_debug_depth=1)
+    best, journal = search.breadth_first_improve(root, "ideas", None, params=params)
+
+    assert len(calls) == 2
+    assert best.debug_depth == 1
+    assert len(journal.nodes) == 1
+
+
+def test_orchestrator_model(monkeypatch):
+    j = search.Journal()
+    n1 = search.PaperNode(Path("a"))
+    n2 = search.PaperNode(Path("b"))
+    n1.score = 1
+    n2.score = 2
+    j.append(n1)
+    j.append(n2)
+
+    captured = {}
+
+    def fake_query(**kwargs):
+        captured["model"] = kwargs.get("model")
+        return {"selected_id": n2.id, "reasoning": ""}
+
+    monkeypatch.setattr(search, "query", fake_query)
+    chosen = j.best_node("orch")
+
+    assert captured["model"] == "orch"
+    assert chosen is n2
diff --git a/tests/test_utils.py b/tests/test_utils.py
new file mode 100644
index 0000000000000000000000000000000000000000..d97a7ad2ce288adeee9b5d6896cb311d7bf6dcfc
--- /dev/null
+++ b/tests/test_utils.py
@@ -0,0 +1,10 @@
+from ai_scientist.paper_improver.utils import unique_subdir
+
+
+def test_unique_subdir(tmp_path):
+    p1 = unique_subdir(tmp_path, "foo")
+    p2 = unique_subdir(tmp_path, "foo")
+    assert p1 != p2
+    assert p1.parent == tmp_path
+    assert not p1.exists()
+    assert not p2.exists()
diff --git a/tests/test_wrappers.py b/tests/test_wrappers.py
new file mode 100644
index 0000000000000000000000000000000000000000..45012ccbb69535fa03052b238636ae601cd62202
--- /dev/null
+++ b/tests/test_wrappers.py
@@ -0,0 +1,46 @@
+import types
+from ai_scientist.paper_improver import llm_review, vlm_review
+
+
+class DummyClient:
+    pass
+
+
+def test_llm_review_model(monkeypatch):
+    called = {}
+
+    def fake_create(model):
+        called['model'] = model
+        return DummyClient(), model
+
+    def fake_perform(path, m, client, **k):
+        called['perform'] = (m, path)
+        return {'ok': True}
+
+    monkeypatch.setattr(llm_review, 'create_client', fake_create)
+    monkeypatch.setattr(llm_review, 'perform_review', fake_perform)
+
+    res = llm_review.llm_review('file.pdf', model='some-model')
+    assert res == {'ok': True}
+    assert called['model'] == 'some-model'
+    assert called['perform'] == ('some-model', 'file.pdf')
+
+
+def test_vlm_review_model(monkeypatch):
+    called = {}
+
+    def fake_create(model):
+        called['model'] = model
+        return DummyClient(), model
+
+    def fake_perform(client, model, path):
+        called['perform'] = (model, path)
+        return {'done': True}
+
+    monkeypatch.setattr(vlm_review, 'create_vlm_client', fake_create)
+    monkeypatch.setattr(vlm_review, 'perform_imgs_cap_ref_review', fake_perform)
+
+    res = vlm_review.vlm_review('doc.pdf', model='vlm-model')
+    assert res == {'done': True}
+    assert called['model'] == 'vlm-model'
+    assert called['perform'] == ('vlm-model', 'doc.pdf')
